{"cells":[{"cell_type":"markdown","metadata":{"id":"3uK2xgy38AtY"},"source":["\n","\n","Import the PIMA Indian dataset from your local drive/Google Drive"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"elapsed":9500,"status":"ok","timestamp":1692115642505,"user":{"displayName":"Bhaswar Chakrabarti","userId":"04250851942391306010"},"user_tz":-330},"id":"_dhDOT0HNfhL","outputId":"9475499c-e5d9-4189-fb3b-d7080d2cff2a"},"outputs":[],"source":["## The following method is for uploading the dataset from a local drive. Change if you are uploading from GDrive\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy\n","from PIL import Image\n","\n","from scipy import ndimage\n","import pandas as pd\n","# from google.colab import files\n","# data_load = files.upload()\n","import io\n","data=pd.read_csv('pima-indians-diabetes.csv')\n","# data = np.array(data)"]},{"cell_type":"markdown","metadata":{"id":"9e1wuDum79mm"},"source":["Check the size of the dataset, i.e. the number of rows and columns"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"H4kv0-d2NfhN"},"outputs":[{"data":{"text/plain":["pandas.core.frame.DataFrame"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["rows = data.shape[0]\n","cols = data.shape[1]\n","rows, cols\n","type(data)"]},{"cell_type":"markdown","metadata":{"id":"A-Sxnm0d9Sn2"},"source":["Next, modify the dataset by removing zero values for \"BloodPressure\", \"BMI\" and \"Glucose\"\n","Then define the independent and the dependent variables (x and y)\n","Finally, split the dataset with training and test subsets\n","Check the sizes of the train and the test datasets"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"zsim-MKCNfhO"},"outputs":[{"data":{"text/plain":["(724, 9)"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["# remove the zero rows\n","data_mod = data[(data.BloodPressure != 0) & (data.BMI != 0) & (data.Glucose != 0)]\n","data_mod.shape"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"KeBmyOyiNfhO"},"outputs":[{"name":"stdout","output_type":"stream","text":["(579, 8)\n","(579,)\n","(145, 8)\n","(145,)\n"]}],"source":["from sklearn.model_selection import train_test_split\n","x = data_mod.iloc[:, :-1]\n","y = data_mod.iloc[:, -1]\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=23)\n","print(x_train.shape)\n","print(y_train.shape)\n","print(x_test.shape)\n","print(y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"4DeFE4IjAeR7"},"source":["Next, we will define the structure of the network:\n","Create a function that takes x and y as the inputs\n","Use two hidden layers aside from the input and the output layer.\n","The function should return the number of units in the input layer, hidden layer 1, hidden layer 2 and the output layer.\n","Pass the training dataset to check the structure of the network.  "]},{"cell_type":"code","execution_count":56,"metadata":{"id":"zKNOdEpbNfhP"},"outputs":[],"source":["## First we will define the structure of the NN:the number of input units, number of hidden units and output units.\n","## Number of input units is equal to the number of features in the dataset\n","## We can choose how many hidden units we want to use\n","## Print the number of units in each layer\n","\n","def nn_structure(x, y, n_hl1, n_hl2):\n","    input_unit = x.shape[1]\n","    hidden_layer1 = n_hl1\n","    hidden_layer2 = n_hl2\n","    output_unit = 1\n","    return input_unit, hidden_layer1, hidden_layer2, output_unit  \n","\n","# input_unit, hidden_layer1, hidden_layer2, output_unit = nn_structure(x, y, 30, 30)  "]},{"cell_type":"markdown","metadata":{"id":"wc88FfJNXEyz"},"source":["Create a function for parameter initialization.\n","The function should take the units in each layer as inputs.\n","It should return the weights and biases for all the layers.\n","Use random initial weights and zero biases.\n"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"CeoiFVsHNfhP"},"outputs":[],"source":["## We then initialize the parameters, i.e the weight and biases for each layer\n","def parameters_initialization(n_in_neurons, n_out_neurons, scale=0.01):\n","    # 8*N sized weights for hidden layer 1\n","    # N*N for hidden layer 2\n","    weights = np.random.random((n_in_neurons, n_out_neurons)) * scale # initialize between 0 and 1\n","    biases = np.zeros((1, n_out_neurons))\n","\n","    return weights, biases\n","\n","# w1, b1 = parameters_initialization(input_unit, hidden_layer1)\n","# w2, b2 = parameters_initialization(hidden_layer1, hidden_layer2)\n","# w3, b3 = parameters_initialization(hidden_layer2, output_unit)\n","# w1.shape, w2.shape, w3.shape, b1.shape, b2.shape, b3.shape"]},{"cell_type":"markdown","metadata":{"id":"iKOwOW-UczbW"},"source":["Next, we define the activation functions"]},{"cell_type":"code","execution_count":58,"metadata":{"executionInfo":{"elapsed":578,"status":"ok","timestamp":1692124035978,"user":{"displayName":"Bhaswar Chakrabarti","userId":"04250851942391306010"},"user_tz":-330},"id":"AgP865dWc3x0"},"outputs":[],"source":["def sigmoid(z):\n","    return 1/(1+np.exp(-z))\n","\n","def relu(z):\n","    return np.maximum(0,z)"]},{"cell_type":"markdown","metadata":{"id":"C64zQyI7c9Zq"},"source":["Create a function to calculate the forward pass.\n","The function should take input x and the network parameters as inputs\n","The function returns all the \"z\" values and the outputs of each layer in a cache"]},{"cell_type":"code","execution_count":59,"metadata":{"id":"uKUGV3DXNfhP"},"outputs":[],"source":["## Next we define the forward pass\n","def forward_propagation(X, w1, w2, w3, b1, b2, b3):\n","    a1 = sigmoid(np.matmul(X, w1) + b1)\n","    a2 = sigmoid(np.matmul(a1, w2) + b2)\n","    Z = np.matmul(a2, w3) + b3\n","    return a1, a2, Z\n","\n","# a1, a2, Z = forward_propagation(x_train, w1, w2, w3, b1, b2, b3)\n","# a1.shape, a2.shape, Z.shape"]},{"cell_type":"markdown","metadata":{"id":"AQfxJPNYdqPD"},"source":["Create a function to calculate the log-loss/cost.\n","The function takes the output of the final layer, y and the parameters as inputs\n","The function returns the calculated cost.\n","Remember that the cost should be calculated over all the training samples."]},{"cell_type":"code","execution_count":60,"metadata":{"id":"7ZJBi4fLNfhQ"},"outputs":[],"source":["def cross_entropy_cost(Z, Y):\n","    # Y is a pandas dataframe with dim (500,)\n","    Y = (np.array(Y)).reshape(Y.shape[0], 1)\n","    Y_hat = sigmoid(Z)\n","    # print(Y.shape, Y_hat.shape)\n","    cost = -np.sum(Y*np.log(Y_hat) + (1-Y)*np.log(1-Y_hat)) / Y.shape[0]\n","    return cost \n","\n","# cross_entropy_cost(Z, y_train)"]},{"cell_type":"markdown","metadata":{"id":"Zbgt0sDofYcX"},"source":["Create a function to calculate the backpropagation.\n","The function takes network parameters, the cache from the function \"forward_propagation\", x and y as inputs.\n","The function should return the gradients, i.e \"dz\", \"dw\" and \"db\" values."]},{"cell_type":"code","execution_count":61,"metadata":{"id":"jBdRQivGNfhQ"},"outputs":[],"source":["def sigmoid_derivative(x):\n","    return sigmoid(x) * (1 - sigmoid(x))\n","\n","def backward_propagation(X, Y, Z, a1, a2, w1, w2, w3):\n","    Y_hat = sigmoid(Z)\n","    dz = Y_hat - np.array(Y).reshape(-1, 1)\n","    m = Y.shape[0]\n","    dw3 = np.matmul(a2.T, (dz*sigmoid_derivative(Z))) / m\n","    dw2 = np.matmul(a1.T, np.matmul(dz*sigmoid_derivative(Z), w3.T)*sigmoid_derivative(a2)) / m\n","    dw1 = np.matmul(X.T, np.matmul(np.matmul(dz*sigmoid_derivative(Z), w3.T)*sigmoid_derivative(a2), w2.T)*sigmoid_derivative(a1)) / m\n","\n","    db3 = np.sum(dz*sigmoid_derivative(Z), axis=0) / m\n","    db2 = np.sum(np.matmul(dz*sigmoid_derivative(Z), w3.T)*sigmoid_derivative(a2), axis=0) / m\n","    db1 = np.sum(np.matmul(np.matmul(dz*sigmoid_derivative(Z), w3.T)*sigmoid_derivative(a2), w2.T)*sigmoid_derivative(a1), axis=0) / m\n","\n","    # reshape db1 db2 db3 to 1x30\n","    db1 = np.array(db1).reshape(1, -1)\n","    db2 = np.array(db2).reshape(1, -1)\n","    db3 = np.array(db3).reshape(1, -1)\n","    \n","    return dw1, dw2, dw3, db1, db2, db3 \n","\n","# dw1, dw2, dw3, db1, db2, db3 = backward_propagation(x_train, y_train, Z, a1, a2, w1, w2, w3)\n","# dw1.shape, dw2.shape, dw3.shape, db1.shape, db2.shape, db3.shape"]},{"cell_type":"markdown","metadata":{"id":"KNklomhxgT_G"},"source":["Create a function to update the parameters.\n","The function takes the parameters, the gradients and the learning rate as inputs.\n","The fucntion returns the parameters after updating their values."]},{"cell_type":"code","execution_count":62,"metadata":{"id":"f5h9Olw8NfhQ"},"outputs":[],"source":["def gradient_descent(w1, w2, w3, b1, b2, b3, dw1, dw2, dw3, db1, db2, db3, alpha):\n","    w1 = w1 - alpha * dw1\n","    w2 = w2 - alpha * dw2\n","    w3 = w3 - alpha * dw3\n","    b1 = b1 - alpha * db1\n","    b2 = b2 - alpha * db2\n","    b3 = b3 - alpha * db3\n","\n","    return w1, w2, w3, b1, b2, b3\n","\n","# w1, w2, w3, b1, b2, b3 = gradient_descent(w1, w2, w3, b1, b2, b3, dw1, dw2, dw3, db1, db2, db3, 0.01)\n","# w1.shape, w2.shape, w3.shape, b1.shape, b2.shape, b3.shape"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["# save the parameters dictionary in a file called 'params.pickle'\n","import pickle\n","def save_model(params):\n","    with open('params.pickle', 'wb') as f:\n","        pickle.dump(params, f)\n","        "]},{"cell_type":"markdown","metadata":{"id":"oX28amKRg4jK"},"source":["Compile the model using a function.\n","It should take x, y, the hidden units and the number of iterations as inputs.\n","It should return the parameters from the gradient_descent function.\n","Print the cost as a function of the number of iterations."]},{"cell_type":"code","execution_count":64,"metadata":{"id":"MkKm3VMPNfhQ"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch =  0 Cost =  0    0.698669\n","dtype: float64\n"]},{"name":"stdout","output_type":"stream","text":["Epoch =  10 Cost =  0    0.677234\n","dtype: float64\n","Epoch =  20 Cost =  0    0.663658\n","dtype: float64\n","Epoch =  30 Cost =  0    0.655035\n","dtype: float64\n","Epoch =  40 Cost =  0    0.649515\n","dtype: float64\n","Epoch =  50 Cost =  0    0.64595\n","dtype: float64\n","Epoch =  60 Cost =  0    0.643625\n","dtype: float64\n","Epoch =  70 Cost =  0    0.642097\n","dtype: float64\n","Epoch =  80 Cost =  0    0.641085\n","dtype: float64\n","Epoch =  90 Cost =  0    0.640409\n","dtype: float64\n","Epoch =  100 Cost =  0    0.639954\n","dtype: float64\n","Epoch =  110 Cost =  0    0.639647\n","dtype: float64\n","Epoch =  120 Cost =  0    0.639436\n","dtype: float64\n","Epoch =  130 Cost =  0    0.639291\n","dtype: float64\n","Epoch =  140 Cost =  0    0.639188\n","dtype: float64\n","Epoch =  150 Cost =  0    0.639113\n","dtype: float64\n","Epoch =  160 Cost =  0    0.639057\n","dtype: float64\n","Epoch =  170 Cost =  0    0.639012\n","dtype: float64\n","Epoch =  180 Cost =  0    0.638975\n","dtype: float64\n","Epoch =  190 Cost =  0    0.638944\n","dtype: float64\n","Epoch =  200 Cost =  0    0.638916\n","dtype: float64\n","Epoch =  210 Cost =  0    0.638891\n","dtype: float64\n","Epoch =  220 Cost =  0    0.638867\n","dtype: float64\n","Epoch =  230 Cost =  0    0.638845\n","dtype: float64\n","Epoch =  240 Cost =  0    0.638825\n","dtype: float64\n"]}],"source":["def neural_network_model(X, Y, n_nodes_hl1, n_nodes_hl2, hm_epochs):\n","    # Define the neural network model\n","    input_unit, hidden_unit1, hidden_unit2, output_unit = nn_structure(X, Y, n_nodes_hl1, n_nodes_hl2)\n","    w1, b1 = parameters_initialization(input_unit, hidden_unit1)\n","    w2, b2 = parameters_initialization(hidden_unit1, hidden_unit2)\n","    w3, b3 = parameters_initialization(hidden_unit2, output_unit)\n","\n","    # hyperparameters\n","    learning_rate = 0.1\n","    threshold = 1e-4\n","    \n","    # Train the neural network model\n","    for epoch in range(hm_epochs):\n","        a1, a2, Z = forward_propagation(X, w1, w2, w3, b1, b2, b3)\n","        cost = cross_entropy_cost(Z, Y)\n","        dw1, dw2, dw3, db1, db2, db3 = backward_propagation(X, Y, Z, a1, a2, w1, w2, w3)\n","        w1, w2, w3, b1, b2, b3 = gradient_descent(w1, w2, w3, b1, b2, b3, dw1, dw2, dw3, db1, db2, db3, learning_rate)\n","\n","        if epoch % 10 == 0:\n","            print('Epoch = ', epoch, 'Cost = ', cost)\n","        \n","        previous_cost = cost\n","\n","        \n","    parameters = {'w1': w1, 'w2': w2, 'w3': w3, 'b1': b1, 'b2': b2, 'b3': b3}\n","    save_model(parameters)\n","    return parameters\n","\n","parameters = neural_network_model(x_train, y_train, 10, 10, 250)"]},{"cell_type":"markdown","metadata":{},"source":["### Test\n","Now test the model against both the train and test datasets."]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["from sklearn.metrics import accuracy_score \n","\n","def predict(X, parameters):\n","    _, _, Z = forward_propagation(X, parameters['w1'], parameters['w2'], parameters['w3'], parameters['b1'], parameters['b2'], parameters['b3'])\n","    Y_prediction = np.where(Z > 0.5, 1, 0)\n","    return Y_prediction\n","\n","def validate(Y_actual, Y_predicted):\n","    accuracy = accuracy_score(Y_actual, Y_predicted)\n","    return accuracy*100\n","    "]},{"cell_type":"code","execution_count":66,"metadata":{"id":"U_kdJJsXNfhQ"},"outputs":[{"data":{"text/plain":["66.32124352331607"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["# Train Data\n","train_accuracy = validate(y_train, predict(x_train, parameters))\n","train_accuracy\n"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[{"data":{"text/plain":["62.758620689655174"]},"execution_count":67,"metadata":{},"output_type":"execute_result"},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["# Test Data\n","test_accuracy = validate(y_test, predict(x_test, parameters))\n","test_accuracy"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
